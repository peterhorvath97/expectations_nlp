---
title: "ML alkalmazások projektfeladat"
author: "Péter Horváth"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  pdf_document:
    #toc: true
    #toc_depth: 3
    includes:
      in_header: "preamble.tex"
    #before_body: "before_body.tex"
    number_sections: true
bibliography: "ref.bib"
csl: "custom-citations.csl"
link-citations: true
linkcolor: "blue"
geometry: "a4paper,outer=25mm,inner=25mm,top=25mm,bottom=25mm"
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      results = FALSE)

library(tidyverse)
library(readr)
library(lubridate)
library(stringr)
library(rebus)
library(tidytext)


fomclinks <- read_rds('../data/fomclinks.rds')
headfoot <- c("\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2}(?:–\\d{1,2})?,\\s+\\d{4}\\b",
  "\\b(?:Sunday|Monday|Tuesday|Wednesday|Thursday|Friday|Saturday)",
  or('a.m.', 'p.m.'),
  one_or_more(DGT) %R% ' of ' %R% one_or_more(DGT)) %>% 
  paste0(collapse = '|')

modelmetrics <- read_rds('../data/modelmetrics.rds')
lda <- read_rds('../data/lda.rds')
idx_df <- read_rds('../data/thematic_uncertainty.rds')

```

A projektfeladatban a Federal Reserve Bank által publikált Federal Open Market Committee (FOMC) ülések transcript-jeit elemzem NLP módszerekkel. Az elemzés célja, hogy feltárjam a kamatdöntő üléseken elhangzott témákat, és egy-egy indexet készítsek, amely időben leköveti ezen témák eloszlását minden egyes ülésre. A projekt a job market paper-öm első lépése, a kapott eredményeket monetáris politikai sokkok identifikációjára fogom használni, "továbbfejlesztve" @aruoba2024identifying tanulmányát, amely hasonló ötleten alapszik, azonban a mögöttes szövegfeldolgozási módszerek egyszerűbbek.

A szükséges adatokat az FOMC oldalairól töltöttem le, ezek: [https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm](https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm) - amely az elmúlt 5 év üléseihez kapcsolódó dokumentumokat tartalmazza; valamint [https://www.federalreserve.gov/monetarypolicy/fomc_historical.htm](https://www.federalreserve.gov/monetarypolicy/fomc_historical.htm) - amely 1939-ig visszamenőleg tartalmazza ezen dokumentumokat (illetve azok elődjeit). A transcript-ek mellet számos más dokumentum típust is publikálnak, amelyek:

 * Transcripts: Az ülés szó szerinti kézirata, amelyek mindig 5 éves csúszással kerülnek publikálásra. 
 * Memorandum of Discussions (Memo): A transcript-ekhez hasonló dokumentum, amely E/3-ban dokumentálja az ülésen elhangzottakat. A transcript-ekkel ellentétben azonban csak néhány évben kerültek publikálásra.
 * Minutes: Ezek több néven is publikálásra kerültek az évek során: FOMC minutes (legfrissebb megnevezés), Historical Minutes és Records of Policy actions. A meeting minutes-ek egy rövidített összefoglalói az ülésen elhangzottaknak. A meeting minutes dokumentumok mindig az ülések után kerülnek publikálásra.
 * Statements: Rövid (1-2 bekezdésnyi) összefoglalók a monetáris politikai döntésekről, melyek azonnal publikálásra kerülnek.
 * "ColorBooks": A Red, Green, Blue, Teal és Beigebook kiadványokat tettem ebbe a kategóriába. Ezek gazdasági és pénzpiaci elemzések, amelyeket a FED Staff készít az ülések előtt (hasonlóak az MNB Inflációs jelenzéséhez).

Az alábbi grafikonon látható a kiadványtípusok időbeli alakulása:

```{=latex}
\begin{figure}[!htbp]
```
```{r fomctypes}
fomclinks %>% 
  mutate(year = year(mtg_date)) %>% 
  group_by(year, name) %>% 
  count() %>%  
  ggplot(aes(x = year, y = n, fill = name)) + 
  geom_col(color = 'black')

```
```{=latex}
\caption{FOMC releases by publication type annually. \label{fig:fomctypes}}

\end{figure}
```

Ebben az elemzésben csak a "Transcript" dokumentumokat használom, később a monetáris sokkok identifikációjához azonban hasznos lehet a "Colorbook"-ok használata is - pl.: a sokk identifikációjára egy lehetséges megoldás a téma-eloszlások különbsége, vagy a témához kötött sentiment értékek különbsége az elemzők és a döntéshozók között. 

A projektfeladatban végzett modellezésre @blei2003latent LDA algoritmusát használom, @hansen2018transparency tanulmányához hasonlóan, akik szintén LDA modellt alkalmaznak az FOMC transcript-ek elemzéséhez. A feladathoz szükséges kódokat R-ben írtam, a következőkben pedig tételesen ismertetem a szövegfeldolgozás és LDA modell tanítás részleteit, végül pedig annak eredményeit.

Szövegfeldolgozás:

 * A letöltött PDF file-okat bekezdésekre tagolom és regex-ek^[`r print(headfoot)`] segítségével eltávolítom belőle a fej- és lábléceket. Ennek eredményeképp minden nem-üres bekezdést véve hozok létre egy corpus-t.
 * Második lépésként a dokumentumokban található szavakat part-of-speech tag-elem @wijffels2021udpipe UDPIPE implementációjának segítségével. 
 * A dokumentumokból egy-, két- és háromszavas (unigram, bigram, trigram) tokeneket képzek, amelyekből a POS tag segítségével specifikus szókapcsolatokat tartok csak meg. Unigram-ek esetén csak főneveket; Bigram-ek esetén melléknév-főnév és főnév-főnév kombinációk, Trigram-ek estén pedig melléknév-melléknév-főnév, melléknév-főnév-főnév, főnév-melléknév-főnév, főnév-főnév-főnév, valamint főnév-preopzíció-főnév. (@justeson1995technical tanulmánya ezeket azonosítja, mint specifikus szókapcsolat-fajták, amelyek képesek jól megragadni különböző témákat.)
 * Végül a token-eket kisbetűsítem, eltávolítom a gyakran ismétlődő (stopword) szavakat, és minden szót annak lingvisztikai gyökerévé transzformálom (wordstem).

Token szelekció:
@hansen2018transparency tanulmánya TF-IDF score alapján szűri a modell illesztéséhez használt token-eket, tapasztalatom szerint ez azonban i) túl szűkre csökkenti a token-ek számát, valamint, ii) eltávolít néhány gazdasági értelemben kulcsfontosságú token-t (pl. "inflat", "growth", "rate") azok alacsony IDF score-ja miatt. Egyszerűsítésképp, az elemzéshez három IDF cutoff értéket választok, ami azon token-eket szűri ki, amelyek a corpus dokumentumainak csak egy extrémen alacsony hányadában szerepelnek. A cutoff értékek 7.6 az unigram tokenek, 8.3 a bigram tokenek és 9 a trigram tokenek esetében. Az értékek rendre 0.05%, 0.025% and 0.0125% inverse document frequency hányadokra vonatkoznak. 


LDA modell:
@hansen2018transparency tanulmányától szintén eltérve, az unsupervised LDA algoritmus helyett egy semi-supervised seeded LDA algoritmust alkalmazok $k = 19$ témával, témánként 5 seedword-del, amelyek:

 * Topic 1 - Inflation: 'inflat expect', 'core inflat', 'cpi', 'price stabil', 'inflationari pressur'
 * Topic 2 - GDP growth: 'gdp growth', 'real gdp', 'output growth', 'potenti gdp', 'potenti output'
 * Topic 3 - Labor Market: 'employ', 'unemploy rate', 'labor market', 'real wage', 'vacanc rate'
 * Topic 4 - Monetary Policy: 'monetari polici', 'polici decis', 'quantit eas', 'polici stanc', 'forward guidanc' 
 * Topic 5 - Fiscal Policy: 'fiscal polici', 'fiscal stimulu', 'budget', 'tax', 'debt limit'
 * Topic 6 - Banks: 'bank system', 'loan', 'credit', 'financi institut', 'leverag'
 * Topic 7 - Yield Curve: 'yield curv', 'term spread', 'bond spread', 'govern bond', 'treasuri yield'
 * Topic 8 - Housing Market: 'real estat', 'mortgag rate', 'properti', 'hous price', 'hous market'
 * Topic 9 - Commodity Markets: 'oil', 'food', 'energi', 'commod price', 'natur ga'
 * Topic 10 - Trade: 'trade', 'import', 'export', 'current account', 'good servic'
 * Topic 11 - Exchange rates: 'currenc', 'exchang', 'appreci', 'depreci', 'foreign central bank'
 * Topic 12 - Recessions: 'recess', 'crisi', 'downturn', 'unemploy rise', 'declin output'
 * Topic 13 - Money Market: 'monei suppli', 'monei growth', 'monei market', 'short rate', 'credit growth'
 * Topic 14 - Asset Purchase: 'balanc sheet', 'asset purchas', 'mb', 'quantit eas', 'open market oper' 
 * Topic 15 - Financial Stability: 'larg bank', 'capit requir', 'stress test', 'system risk', 'liquid risk'
 * Topic 16 - Financial Markets: 'stock market', 'equiti price', 'corpor bond', 'financi market volatil', 'risk return'
 * Topic 17 - Consumer Confidence: 'consum confid', 'consum sentiment', 'retail sale', 'household spend', 'auto sale'
 * Topic 18 - Policy Rate: 'fund rate', 'feder fund rate', 'interest rate', 'real rate', 'natur rate'
 * Topic 19 - Domestic Businesses: 'small busi', 'inventori', 'labor cost', 'larg firm', 'busi loan'

@hansen2018transparency $k = 40$ témával illesztett LDA modellt, így az optimális középpontszám megtalálása érdekében $k = [19,40]$ intervallumon tanítok modelleket. Az végső $k$ értéket pedgig perplexity és coherence score-ok alapján választom. Az LDA modell hiperparamétereit $\alpha = 50/k$ és $\eta = 0.025$ kalibrálom a korábbi tanulmányt követve. Az LDA modelleket Gibbs sampling-gel becsülöm, 100 iterácíóval és 10 burnin-nel.

```{=latex}
\begin{figure}[!htbp]
```
```{r modelmetrics}
modelmetrics %>% 
  drop_na() %>% 
  ggplot(aes(x = k, y = value)) +
  geom_line() +
  facet_wrap(~score, scales = 'free')

```
```{=latex}
\caption{LDA evaluation metrics. \label{fig:modelmetrics}}

\end{figure}
```

Nem meglepően, a perplexity score a lehető legnagyobb klaszter középpont számot javasolja, az átlagos topic-coherence score azonban a témák számával csökken, így $k = 20$^[Kellő koherencia híjján a 20 témát 'Other'-ként címkéztem fel.] középponttal illesztem az elemzésre használt LDA modellt. A kapott modell $\beta$ mátrixa megadja a token-ek témák közti eloszlását. Az alábbi grafikonon minden témához a 12 legvalószínűbb token-t ábrázolom.


\newpage
```{=latex}
\begin{figure}[!htbp]
```
```{r topicdist, fig.height=20, fig.width=20, out.extra='angle=90'}
labels <- tibble(topic = c(1:20),
                 theme = c('Inflation',
                           'GDP Growth',
                           'Labor Market',
                           'Monetary Policy',
                           'Fiscal Policy',
                           'Banks',
                           'Yield Curve',
                           'Housing Market',
                           'Commodity Prices',
                           'International Trade',
                           'Exchange Rates',
                           'Recessions',
                           'Money Market',
                           'Asset Purchases',
                           'Financial Stability',
                           'Financial Markets', 
                           'Consumer Confidence',
                           'Policy Rate',
                           'Domestic Businesses',
                           'Other'))


tidy(lda, matrix = 'beta') %>% 
  inner_join(labels) %>% 
  mutate(topic = theme) %>% 
  group_by(topic) %>% 
  top_n(12, beta) %>% 
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered() +
  theme(text = element_text(size=15))
```
```{=latex}
\caption{LDA evaluation metrics. \label{fig:topicdist}}

\end{figure}
```
\newpage

AZ LDA modellel továbbá megbecsülhető a dokumentumok téma-eloszlása is, melyet kiátlagolva származtaható, hogy egy adott publikációkban az egyes érintett témák az adott ülésben mennyire voltak hangsúlyosak. Az alábbi ábrán ennek időbeli alakulása látható.^[A művelet ugyanúgy elvégezhető a többi publikációs típussal is - ezt elvégeztem, így az adatfile-jaimban ezek is megtalálhatók, itt csak a Transcript-ekkel készített idősort ábrázolom.] 


```{=latex}
\begin{figure}[!htbp]
```
```{r topicdist_overtime, fig.height=20, fig.width=20, out.extra='angle=90'}

idx_df  %>%
  filter(name == 'Transcript') %>% 
  ggplot(aes(x = mtg_date, y = topic_prop, color = topic)) +
  geom_line() +
  facet_wrap(~topic, scales = 'free')


```
```{=latex}
\caption{LDA evaluation metrics. \label{fig:topicdist_overtime}}

\end{figure}
```



\newpage

# References

```{=tex}
\bgroup
\renewcommand{\baselinestretch}{1}\normalsize
```
::: {#refs}
:::

::: {#ref}
:::

```{=tex}
\egroup
\newpage
```